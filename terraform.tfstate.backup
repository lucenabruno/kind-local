{
  "version": 4,
  "terraform_version": "1.0.1",
  "serial": 118,
  "lineage": "e0966de8-f3dd-811d-3ab6-602681be5473",
  "outputs": {},
  "resources": [
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "cert_manager",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "cert-manager",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "cert-manager",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 10,
            "metadata": [
              {
                "app_version": "v1.5.3",
                "chart": "cert-manager",
                "name": "cert-manager",
                "namespace": "cert-manager",
                "revision": 1,
                "values": "{\"cainjector\":{\"containerSecurityContext\":{}},\"containerSecurityContext\":{},\"featureGates\":\"\",\"global\":{\"imagePullSecrets\":[],\"leaderElection\":{\"namespace\":\"kube-system\"},\"logLevel\":2,\"podSecurityPolicy\":{\"enabled\":false,\"useAppArmor\":true},\"priorityClassName\":\"\",\"rbac\":{\"create\":true}},\"installCRDs\":true,\"prometheus\":{\"enabled\":true,\"servicemonitor\":{\"enabled\":false,\"interval\":\"60s\",\"labels\":{},\"path\":\"/metrics\",\"prometheusInstance\":\"default\",\"scrapeTimeout\":\"30s\",\"targetPort\":9402}},\"securityContext\":{\"runAsNonRoot\":true},\"startupapicheck\":{\"backoffLimit\":4,\"enabled\":true,\"securityContext\":{\"runAsNonRoot\":true},\"timeout\":\"1m\"},\"strategy\":{},\"webhook\":{\"containerSecurityContext\":{},\"hostNetwork\":false,\"replicaCount\":1,\"securityContext\":{\"runAsNonRoot\":true},\"serviceType\":\"ClusterIP\",\"timeoutSeconds\":10,\"url\":{}}}",
                "version": "v1.5.3"
              }
            ],
            "name": "cert-manager",
            "namespace": "cert-manager",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://charts.jetstack.io",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "global:\n  ## Reference to one or more secrets to be used when pulling images\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ##\n  imagePullSecrets: []\n  # - name: \"image-pull-secret\"\n\n  # Optional priority class to be used for the cert-manager pods\n  priorityClassName: \"\"\n  rbac:\n    create: true\n\n  podSecurityPolicy:\n    enabled: false\n    useAppArmor: true\n\n  # Set the verbosity of cert-manager. Range of 0 - 6 with 6 being the most verbose.\n  logLevel: 2\n\n  leaderElection:\n    # Override the namespace used to store the ConfigMap for leader election\n    namespace: \"kube-system\"\n\n    # The duration that non-leader candidates will wait after observing a\n    # leadership renewal until attempting to acquire leadership of a led but\n    # unrenewed leader slot. This is effectively the maximum duration that a\n    # leader can be stopped before it is replaced by another candidate.\n    # leaseDuration: 60s\n\n    # The interval between attempts by the acting master to renew a leadership\n    # slot before it stops leading. This must be less than or equal to the\n    # lease duration.\n    # renewDeadline: 40s\n\n    # The duration the clients should wait between attempting acquisition and\n    # renewal of a leadership.\n    # retryPeriod: 15s\n\ninstallCRDs: true\n\nstrategy: {}\n  # type: RollingUpdate\n  # rollingUpdate:\n  #   maxSurge: 0\n  #   maxUnavailable: 1\n\n# Comma separated list of feature gates that should be enabled on the\n# controller pod.\nfeatureGates: \"\"\n\n# Pod Security Context\n# ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\nsecurityContext:\n  runAsNonRoot: true\n# legacy securityContext parameter format: if enabled is set to true, only fsGroup and runAsUser are supported\n# securityContext:\n#   enabled: false\n#   fsGroup: 1001\n#   runAsUser: 1001\n# to support additional securityContext parameters, omit the `enabled` parameter and simply specify the parameters\n# you want to set, e.g.\n# securityContext:\n#   fsGroup: 1000\n#   runAsUser: 1000\n#   runAsNonRoot: true\n\n# Container Security Context to be set on the controller component container\n# ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\ncontainerSecurityContext: {}\n  # capabilities:\n  #   drop:\n  #   - ALL\n  # readOnlyRootFilesystem: true\n  # runAsNonRoot: true\n\nprometheus:\n  enabled: true\n  servicemonitor:\n    enabled: false\n    prometheusInstance: default\n    targetPort: 9402\n    path: /metrics\n    interval: 60s\n    scrapeTimeout: 30s\n    labels: {}\n\nwebhook:\n  replicaCount: 1\n  timeoutSeconds: 10\n\n  # Pod Security Context to be set on the webhook component Pod\n  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n  securityContext:\n    runAsNonRoot: true\n\n  # Container Security Context to be set on the webhook component container\n  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n  containerSecurityContext: {}\n    # capabilities:\n    #   drop:\n    #   - ALL\n    # readOnlyRootFilesystem: true\n    # runAsNonRoot: true\n\n  # Specifies if the webhook should be started in hostNetwork mode.\n  #\n  # Required for use in some managed kubernetes clusters (such as AWS EKS) with custom\n  # CNI (such as calico), because control-plane managed by AWS cannot communicate\n  # with pods' IP CIDR and admission webhooks are not working\n  #\n  # Since the default port for the webhook conflicts with kubelet on the host\n  # network, `webhook.securePort` should be changed to an available port if\n  # running in hostNetwork mode.\n  hostNetwork: false\n\n  # Specifies how the service should be handled. Useful if you want to expose the\n  # webhook to outside of the cluster. In some cases, the control plane cannot\n  # reach internal services.\n  serviceType: ClusterIP\n  # loadBalancerIP:\n\n  # Overrides the mutating webhook and validating webhook so they reach the webhook\n  # service using the `url` field instead of a service.\n  url: {}\n    # host:\n\ncainjector:\n  # Container Security Context to be set on the cainjector component container\n  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n  containerSecurityContext: {}\n    # capabilities:\n    #   drop:\n    #   - ALL\n    # readOnlyRootFilesystem: true\n    # runAsNonRoot: true\n\n\n# This startupapicheck is a Helm post-install hook that waits for the webhook\n# endpoints to become available.\nstartupapicheck:\n  enabled: true\n\n  # Pod Security Context to be set on the startupapicheck component Pod\n  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n  securityContext:\n    runAsNonRoot: true\n\n  # Timeout for 'kubectl check api' command\n  timeout: 1m\n\n  # Job backoffLimit\n  backoffLimit: 4\n"
            ],
            "verify": false,
            "version": "v1.5.3",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "kubernetes_namespace.cert_manager"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "cilium",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "status": "tainted",
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "cilium",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "cilium",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 10,
            "metadata": [
              {
                "app_version": "1.10.3",
                "chart": "cilium",
                "name": "cilium",
                "namespace": "kube-system",
                "revision": 1,
                "values": "{\"affinity\":{\"nodeAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":{\"nodeSelectorTerms\":[{\"matchExpressions\":[{\"key\":\"kubernetes.io/os\",\"operator\":\"In\",\"values\":[\"linux\"]}]},{\"matchExpressions\":[{\"key\":\"beta.kubernetes.io/os\",\"operator\":\"In\",\"values\":[\"linux\"]}]}]}},\"podAntiAffinity\":{\"requiredDuringSchedulingIgnoredDuringExecution\":[{\"labelSelector\":{\"matchExpressions\":[{\"key\":\"k8s-app\",\"operator\":\"In\",\"values\":[\"cilium\"]}]},\"topologyKey\":\"kubernetes.io/hostname\"}]}},\"autoDirectNodeRoutes\":false,\"bandwidthManager\":false,\"bgp\":{\"announce\":{\"loadbalancerIP\":false},\"enabled\":false},\"bpf\":{\"clockProbe\":false,\"lbExternalClusterIP\":false,\"lbMapMax\":65536,\"masquerade\":true,\"monitorAggregation\":\"medium\",\"monitorFlags\":\"all\",\"monitorInterval\":\"5s\",\"policyMapMax\":16384,\"preallocateMaps\":false,\"waitForMount\":false},\"certgen\":{\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/cilium/certgen\",\"tag\":\"v0.1.4\"},\"podLabels\":{},\"ttlSecondsAfterFinished\":1800},\"cgroup\":{\"autoMount\":{\"enabled\":true},\"hostRoot\":\"/run/cilium/cgroupv2\"},\"cleanBpfState\":false,\"cleanState\":false,\"clustermesh\":{\"apiserver\":{\"service\":{\"annotations\":{},\"nodePort\":32379,\"type\":\"NodePort\"},\"tls\":{\"admin\":{\"cert\":\"\",\"key\":\"\"},\"auto\":{\"certValidityDuration\":1095,\"enabled\":true,\"method\":\"helm\"},\"ca\":{\"cert\":\"\",\"key\":\"\"},\"client\":{\"cert\":\"\",\"key\":\"\"},\"remote\":{\"cert\":\"\",\"key\":\"\"},\"server\":{\"cert\":\"\",\"key\":\"\"}}},\"useAPIServer\":false},\"cni\":{\"binPath\":\"/opt/cni/bin\",\"chainingMode\":\"none\",\"confFileMountPath\":\"/tmp/cni-configuration\",\"confPath\":\"/etc/cni/net.d\",\"configMapKey\":\"cni-config\",\"customConf\":false,\"exclusive\":true,\"hostConfDirMountPath\":\"/host/etc/cni/net.d\",\"install\":true},\"containerRuntime\":{\"integration\":\"docker\"},\"customCalls\":{\"enabled\":false},\"daemon\":{\"runPath\":\"/var/run/cilium\"},\"datapathMode\":\"veth\",\"debug\":{\"enabled\":false},\"egressGateway\":{\"enabled\":false},\"enableCnpStatusUpdates\":false,\"enableK8sEventHandover\":false,\"enableXTSocketFallback\":true,\"encryption\":{\"enabled\":false,\"interface\":\"\",\"ipsec\":{\"interface\":\"\",\"keyFile\":\"\",\"mountPath\":\"\",\"secretName\":\"\"},\"keyFile\":\"keys\",\"mountPath\":\"/etc/ipsec\",\"nodeEncryption\":false,\"secretName\":\"cilium-ipsec-keys\",\"type\":\"ipsec\"},\"endpointHealthChecking\":{\"enabled\":true},\"endpointRoutes\":{\"enabled\":false},\"endpointStatus\":{\"enabled\":false,\"status\":\"\"},\"eni\":{\"awsReleaseExcessIPs\":false,\"ec2APIEndpoint\":\"\",\"enabled\":false,\"eniTags\":{},\"iamRole\":\"\",\"subnetIDsFilter\":\"\",\"subnetTagsFilter\":\"\",\"updateEC2AdapterLimitViaAPI\":false},\"externalIPs\":{\"enabled\":true},\"gke\":{\"enabled\":false},\"healthChecking\":true,\"healthPort\":9876,\"hostFirewall\":false,\"hostPort\":{\"enabled\":true},\"hostServices\":{\"enabled\":true,\"protocols\":\"tcp,udp\"},\"hubble\":{\"enabled\":true,\"listenAddress\":\":4244\",\"metrics\":{\"enabled\":null,\"port\":9091,\"serviceMonitor\":{\"enabled\":true}},\"relay\":{\"dialTimeout\":null,\"enabled\":true,\"retryTimeout\":null,\"rollOutPods\":false,\"sortBufferDrainTimeout\":null,\"sortBufferLenMax\":null,\"tls\":{\"client\":{\"cert\":\"\",\"key\":\"\"},\"server\":{\"cert\":\"\",\"enabled\":false,\"key\":\"\"}}},\"socketPath\":\"/var/run/cilium/hubble.sock\",\"tls\":{\"auto\":{\"certValidityDuration\":1095,\"enabled\":true,\"method\":\"helm\",\"schedule\":\"0 0 1 */4 *\"},\"ca\":{\"cert\":\"\",\"key\":\"\"},\"enabled\":true,\"server\":{\"cert\":\"\",\"key\":\"\"}},\"ui\":{\"enabled\":true,\"ingress\":{\"annotations\":{\"kubernetes.io/ingress.class\":\"kong\"},\"enabled\":true,\"hosts\":[\"hubble.local\"],\"tls\":[]}}},\"installIptablesRules\":true,\"installNoConntrackIptablesRules\":false,\"ipMasqAgent\":{\"enabled\":false},\"ipam\":{\"mode\":\"kubernetes\",\"operator\":{\"clusterPoolIPv4MaskSize\":24,\"clusterPoolIPv4PodCIDR\":\"10.0.0.0/8\",\"clusterPoolIPv6MaskSize\":120,\"clusterPoolIPv6PodCIDR\":\"fd00::/104\"}},\"k8s\":{},\"k8sServiceHost\":\"172.18.0.3\",\"k8sServicePort\":6443,\"kubeProxyReplacement\":\"strict\",\"l7Proxy\":true,\"logSystemLoad\":false,\"maglev\":{},\"monitor\":{\"enabled\":false},\"nodePort\":{\"enabled\":true},\"nodeinit\":{\"enabled\":true,\"priorityClassName\":\"\"},\"operator\":{\"enabled\":true,\"prometheus\":{\"enabled\":false,\"port\":6942,\"serviceMonitor\":{\"enabled\":true}},\"rollOutPods\":false},\"policyEnforcementMode\":\"default\",\"pprof\":{\"enabled\":false},\"prometheus\":{\"enabled\":false,\"metrics\":null,\"port\":9090,\"serviceMonitor\":{\"enabled\":true}},\"proxy\":{\"prometheus\":{\"enabled\":true,\"port\":\"9095\"},\"sidecarImageRegex\":\"cilium/istio_proxy\"},\"remoteNodeIdentity\":true,\"resourceQuotas\":{\"cilium\":{\"hard\":{\"pods\":\"10k\"}},\"enabled\":false,\"operator\":{\"hard\":{\"pods\":\"15\"}}},\"rollOutCiliumPods\":false,\"sockops\":{\"enabled\":false},\"startupProbe\":{\"failureThreshold\":105,\"periodSeconds\":2},\"tls\":{\"enabled\":true,\"secretsBackend\":\"local\"},\"tunnel\":\"vxlan\",\"wellKnownIdentities\":{\"enabled\":false}}",
                "version": "1.10.3"
              }
            ],
            "name": "cilium",
            "namespace": "kube-system",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://helm.cilium.io/",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "failed",
            "timeout": 300,
            "values": [
              "debug:\n  # -- Enable debug logging\n  enabled: false\n  # verbose:\n\n# -- Roll out cilium agent pods automatically when configmap is updated.\nrollOutCiliumPods: false\n\nk8sServiceHost: 172.18.0.3\nk8sServicePort: 6443\n\n# -- Pod affinity for cilium-agent.\naffinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n        - matchExpressions:\n            - key: kubernetes.io/os\n              operator: In\n              values:\n                - linux\n        # Compatible with Kubernetes 1.12.x and 1.13.x\n        - matchExpressions:\n            - key: beta.kubernetes.io/os\n              operator: In\n              values:\n                - linux\n  podAntiAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n    - labelSelector:\n        matchExpressions:\n        - key: k8s-app\n          operator: In\n          values:\n          - cilium\n      topologyKey: kubernetes.io/hostname\n\n# Configuration Values for cilium-agent\n\n# -- Enable installation of PodCIDR routes between worker\n# nodes if worker nodes share a common L2 network segment.\nautoDirectNodeRoutes: false\n\n# -- Optimize TCP and UDP workloads and enable rate-limiting traffic from\n# individual Pods with EDT (Earliest Departure Time)\n# through the \"kubernetes.io/egress-bandwidth\" Pod annotation.\nbandwidthManager: false\n\n# -- Configure BGP\nbgp:\n  # -- Enable BGP support inside Cilium; embeds a new ConfigMap for BGP inside\n  # cilium-agent and cilium-operator\n  enabled: false\n  announce:\n    # -- Enable allocation and announcement of service LoadBalancer IPs\n    loadbalancerIP: false\n\nbpf:\n  # -- Enable BPF clock source probing for more efficient tick retrieval.\n  clockProbe: false\n\n  # -- Force the cilium-agent DaemonSet to wait in an initContainer until the\n  # eBPF filesystem has been mounted.\n  waitForMount: false\n\n  # -- Enables pre-allocation of eBPF map values. This increases\n  # memory usage but can reduce latency.\n  preallocateMaps: false\n\n  # -- Configure the maximum number of entries in the TCP connection tracking\n  # table.\n  # ctTcpMax: '524288'\n\n  # -- Configure the maximum number of entries for the non-TCP connection\n  # tracking table.\n  # ctAnyMax: '262144'\n\n  # -- Configure the maximum number of service entries in the\n  # load balancer maps.\n  lbMapMax: 65536\n\n  # -- Configure the maximum number of entries for the NAT table.\n  # natMax: 524288\n\n  # -- Configure the maximum number of entries for the neighbor table.\n  # neighMax: 524288\n\n  # -- Configure the maximum number of entries in endpoint policy map (per endpoint).\n  policyMapMax: 16384\n\n  # -- Configure auto-sizing for all BPF maps based on available memory.\n  # ref: https://docs.cilium.io/en/v1.9/concepts/ebpf/maps/#ebpf-maps\n  #mapDynamicSizeRatio: 0.0025\n\n  # -- Configure the level of aggregation for monitor notifications.\n  # Valid options are none, low, medium, maximum.\n  monitorAggregation: medium\n\n  # -- Configure the typical time between monitor notifications for\n  # active connections.\n  monitorInterval: \"5s\"\n\n  # -- Configure which TCP flags trigger notifications when seen for the\n  # first time in a connection.\n  monitorFlags: \"all\"\n\n  # -- Allow cluster external access to ClusterIP services.\n  lbExternalClusterIP: false\n\n  # -- Enable native IP masquerade support in eBPF\n  masquerade: true # kind\n\n  # -- Configure whether direct routing mode should route traffic via\n  # host stack (true) or directly and more efficiently out of BPF (false) if\n  # the kernel supports it. The latter has the implication that it will also\n  # bypass netfilter in the host namespace.\n  #hostRouting: true\n\n  # -- Configure the eBPF-based TPROXY to reduce reliance on iptables rules\n  # for implementing Layer 7 policy.\n  # tproxy: true\n\n  # -- Configure the FIB lookup bypass optimization for nodeport reverse\n  # NAT handling.\n  # lbBypassFIBLookup: true\n\n# -- Clean all eBPF datapath state from the initContainer of the cilium-agent\n# DaemonSet.\n#\n# WARNING: Use with care!\ncleanBpfState: false\n\n# -- Clean all local Cilium state from the initContainer of the cilium-agent\n# DaemonSet. Implies cleanBpfState: true.\n#\n# WARNING: Use with care!\ncleanState: false\n\ncni:\n  # -- Install the CNI configuration and binary files into the filesystem.\n  install: true\n\n  # -- Configure chaining on top of other CNI plugins. Possible values:\n  #  - none\n  #  - generic-veth\n  #  - aws-cni\n  #  - portmap\n  chainingMode: none\n\n  # -- Make Cilium take ownership over the `/etc/cni/net.d` directory on the\n  # node, renaming all non-Cilium CNI configurations to `*.cilium_bak`.\n  # This ensures no Pods can be scheduled using other CNI plugins during Cilium\n  # agent downtime.\n  exclusive: true\n\n  # -- Skip writing of the CNI configuration. This can be used if\n  # writing of the CNI configuration is performed by external automation.\n  customConf: false\n\n  # -- Configure the path to the CNI configuration directory on the host.\n  confPath: /etc/cni/net.d\n\n  # -- Configure the path to the CNI binary directory on the host.\n  binPath: /opt/cni/bin\n\n  # -- Specify the path to a CNI config to read from on agent start.\n  # This can be useful if you want to manage your CNI\n  # configuration outside of a Kubernetes environment. This parameter is\n  # mutually exclusive with the 'cni.configMap' parameter.\n  # readCniConf: /host/etc/cni/net.d/05-cilium.conf\n\n  # -- When defined, configMap will mount the provided value as ConfigMap and\n  # interpret the cniConf variable as CNI configuration file and write it\n  # when the agent starts up\n  # configMap: cni-configuration\n\n  # -- Configure the key in the CNI ConfigMap to read the contents of\n  # the CNI configuration from.\n  configMapKey: cni-config\n\n  # -- Configure the path to where to mount the ConfigMap inside the agent pod.\n  confFileMountPath: /tmp/cni-configuration\n\n  # -- Configure the path to where the CNI configuration directory is mounted\n  # inside the agent pod.\n  hostConfDirMountPath: /host/etc/cni/net.d\n\n# -- Configure how frequently garbage collection should occur for the datapath\n# connection tracking table.\n# conntrackGCInterval: \"0s\"\n\n# -- Configure container runtime specific integration.\ncontainerRuntime:\n  # -- Enables specific integrations for container runtimes.\n  # Supported values:\n  # - containerd\n  # - crio\n  # - docker\n  # - none\n  # - auto (automatically detect the container runtime)\n  integration: docker # kind\n  # -- Configure the path to the container runtime control socket.\n  # socketPath: /path/to/runtime.sock\n\n# crdWaitTimeout: \"\"\n\n# -- Tail call hooks for custom eBPF programs.\ncustomCalls:\n  # -- Enable tail call hooks for custom eBPF programs.\n  enabled: false\n\n# -- Configure which datapath mode should be used for configuring container\n# connectivity. Valid options are \"veth\" or \"ipvlan\".\ndatapathMode: veth\n\ndaemon:\n  # -- Configure where Cilium runtime state should be stored.\n  runPath: \"/var/run/cilium\"\n\n# -- Specify which network interfaces can run the eBPF datapath. This means\n# that a packet sent from a pod to a destination outside the cluster will be\n# masqueraded (to an output device IPv4 address), if the output device runs the\n# program. When not specified, probing will automatically detect devices.\n# devices: \"\"\n\n# -- Chains to ignore when installing feeder rules.\n# disableIptablesFeederRules: \"\"\n\n# -- Limit egress masquerading to interface selector.\n# egressMasqueradeInterfaces: \"\"\n\n# -- Whether to enable CNP status updates.\nenableCnpStatusUpdates: false\n\n# -- Configures the use of the KVStore to optimize Kubernetes event handling by\n# mirroring it into the KVstore for reduced overhead in large clusters.\nenableK8sEventHandover: false\n\n# TODO: Add documentation\n# enableIdentityMark: false\n\n# enableK8sEndpointSlice: false\n\n# -- Enables the fallback compatibility solution for when the xt_socket kernel\n# module is missing and it is needed for the datapath L7 redirection to work\n# properly. See documentation for details on when this can be disabled:\n# http://docs.cilium.io/en/stable/install/system_requirements/#admin-kernel-version.\nenableXTSocketFallback: true\n\nencryption:\n  # -- Enable transparent network encryption.\n  enabled: false\n\n  # -- Encryption method. Can be either ipsec or wireguard.\n  type: ipsec\n\n  # -- Enable encryption for pure node to node traffic.\n  # This option is only effective when encryption.type is set to ipsec.\n  nodeEncryption: false\n\n  ipsec:\n    # -- Name of the key file inside the Kubernetes secret configured via secretName.\n    keyFile: \"\"\n\n    # -- Path to mount the secret inside the Cilium pod.\n    mountPath: \"\"\n\n    # -- Name of the Kubernetes secret containing the encryption keys.\n    secretName: \"\"\n\n    # -- The interface to use for encrypted traffic.\n    interface: \"\"\n\n  # -- Deprecated in favor of encryption.ipsec.keyFile.\n  # Name of the key file inside the Kubernetes secret configured via secretName.\n  # This option is only effective when encryption.type is set to ipsec.\n  keyFile: keys\n\n  # -- Deprecated in favor of encryption.ipsec.mountPath.\n  # Path to mount the secret inside the Cilium pod.\n  # This option is only effective when encryption.type is set to ipsec.\n  mountPath: /etc/ipsec\n\n  # -- Deprecated in favor of encryption.ipsec.secretName.\n  # Name of the Kubernetes secret containing the encryption keys.\n  # This option is only effective when encryption.type is set to ipsec.\n  secretName: cilium-ipsec-keys\n\n  # -- Deprecated in favor of encryption.ipsec.interface.\n  # The interface to use for encrypted traffic.\n  # This option is only effective when encryption.type is set to ipsec.\n  interface: \"\"\n\nendpointHealthChecking:\n  # -- Enable connectivity health checking between virtual endpoints.\n  enabled: true\n\n# -- Enable endpoint status.\n# Status can be: policy, health, controllers, logs and / or state. For 2 or more options use a comma.\nendpointStatus:\n  enabled: false\n  status: \"\"\n\nendpointRoutes:\n  # -- Enable use of per endpoint routes instead of routing via\n  # the cilium_host interface.\n  enabled: false\n\neni:\n  # -- Enable Elastic Network Interface (ENI) integration.\n  enabled: false\n  # -- Update ENI Adapter limits from the EC2 API\n  updateEC2AdapterLimitViaAPI: false\n  # -- Release IPs not used from the ENI\n  awsReleaseExcessIPs: false\n  # -- EC2 API endpoint to use\n  ec2APIEndpoint: \"\"\n  # -- Tags to apply to the newly created ENIs\n  eniTags: {}\n  # -- If using IAM role for Service Accounts will not try to\n  # inject identity values from cilium-aws kubernetes secret.\n  # Adds annotation to service account if managed by Helm.\n  # See https://github.com/aws/amazon-eks-pod-identity-webhook\n  iamRole: \"\"\n  # -- Filter via subnet IDs which will dictate which subnets are going to be used to create new ENIs\n  subnetIDsFilter: \"\"\n  # -- Filter via tags (k=v) which will dictate which subnets are going to be used to create new ENIs\n  subnetTagsFilter: \"\"\n\nexternalIPs:\n  # -- Enable ExternalIPs service support.\n  enabled: true # kind\n\n# fragmentTracking enables IPv4 fragment tracking support in the datapath.\n# fragmentTracking: true\n\ngke:\n  # -- Enable Google Kubernetes Engine integration\n  enabled: false\n\n# -- Enable connectivity health checking.\nhealthChecking: true\n\n# -- TCP port for the agent health API. This is not the port for cilium-health.\nhealthPort: 9876\n\n# -- Enables the enforcement of host policies in the eBPF datapath.\nhostFirewall: false\n\nhostPort:\n  # -- Enable hostPort service support.\n  enabled: true # kind\n\n# -- Configure ClusterIP service handling in the host namespace (the node).\nhostServices:\n  # -- Enable host reachable services.\n  enabled: true # kind\n\n  # -- Supported list of protocols to apply ClusterIP translation to.\n  protocols: tcp,udp\n\n\n# -- Configure certificate generation for Hubble integration.\n# If hubble.tls.auto.method=cronJob, these values are used\n# for the Kubernetes CronJob which will be scheduled regularly to\n# (re)generate any certificates not provided manually.\ncertgen:\n  image:\n    repository: quay.io/cilium/certgen\n    tag: v0.1.4\n    pullPolicy: IfNotPresent\n  # -- Seconds after which the completed job pod will be deleted\n  ttlSecondsAfterFinished: 1800\n  # -- Labels to be added to hubble-certgen pods\n  podLabels: {}\n\nhubble:\n  # -- Enable Hubble (true by default).\n  enabled: true\n\n  # -- Buffer size of the channel Hubble uses to receive monitor events. If this\n  # value is not set, the queue size is set to the default monitor queue size.\n  # eventQueueSize: \"\"\n\n  # -- Number of recent flows for Hubble to cache. Defaults to 4095.\n  # Possible values are:\n  #   1, 3, 7, 15, 31, 63, 127, 255, 511, 1023,\n  #   2047, 4095, 8191, 16383, 32767, 65535\n  # eventBufferCapacity: \"4095\"\n\n  # -- Hubble metrics configuration.\n  # See https://docs.cilium.io/en/stable/configuration/metrics/#hubble-metrics\n  # for more comprehensive documentation about Hubble metrics.\n  metrics:\n    # -- Configures the list of metrics to collect. If empty or null, metrics\n    # are disabled.\n    # Example:\n    #\n    #   enabled:\n    #   - dns:query;ignoreAAAA\n    #   - drop\n    #   - tcp\n    #   - flow\n    #   - icmp\n    #   - http\n    #\n    # You can specify the list of metrics from the helm CLI:\n    #\n    #   --set metrics.enabled=\"{dns:query;ignoreAAAA,drop,tcp,flow,icmp,http}\"\n    #\n    enabled: ~\n    # -- Configure the port the hubble metric server listens on.\n    port: 9091\n    serviceMonitor:\n      # -- Create ServiceMonitor resources for Prometheus Operator.\n      # This requires the prometheus CRDs to be available.\n      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)\n      enabled: true\n\n  # -- Unix domain socket path to listen to when Hubble is enabled.\n  socketPath: /var/run/cilium/hubble.sock\n\n  # -- An additional address for Hubble to listen to.\n  # Set this field \":4244\" if you are enabling Hubble Relay, as it assumes that\n  # Hubble is listening on port 4244.\n  listenAddress: \":4244\"\n\n  # -- TLS configuration for Hubble\n  tls:\n    # -- Enable mutual TLS for listenAddress. Setting this value to false is\n    # highly discouraged as the Hubble API provides access to potentially\n    # sensitive network flow metadata and is exposed on the host network.\n    enabled: true\n    # -- Configure automatic TLS certificates generation.\n    auto:\n      # -- Auto-generate certificates.\n      # When set to true, automatically generate a CA and certificates to\n      # enable mTLS between Hubble server and Hubble Relay instances. If set to\n      # false, the certs for Hubble server need to be provided by setting\n      # appropriate values below.\n      enabled: true\n      # -- Set the method to auto-generate certificates. Supported values:\n      # - helm:      This method uses Helm to generate all certificates.\n      # - cronJob:   This method uses a Kubernetes CronJob the generate any\n      #              certificates not provided by the user at installation\n      #              time.\n      method: helm\n      # -- Generated certificates validity duration in days.\n      certValidityDuration: 1095\n      # -- Schedule for certificates regeneration (regardless of their expiration date).\n      # Only used if method is \"cronJob\". If nil, then no recurring job will be created.\n      # Instead, only the one-shot job is deployed to generate the certificates at\n      # installation time.\n      #\n      # Defaults to midnight of the first day of every fourth month. For syntax, see\n      # https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#schedule\n      schedule: \"0 0 1 */4 *\"\n    # -- base64 encoded PEM values for the Hubble CA certificate and private key.\n    ca:\n      cert: \"\"\n      # -- The CA private key (optional). If it is provided, then it will be\n      # used by hubble.tls.auto.method=cronJob to generate all other certificates.\n      # Otherwise, a ephemeral CA is generated if hubble.tls.auto.enabled=true.\n      key: \"\"\n    # -- base64 encoded PEM values for the Hubble server certificate and private key\n    server:\n      cert: \"\"\n      key: \"\"\n\n  relay:\n    # -- Enable Hubble Relay (requires hubble.enabled=true)\n    enabled: true # kind\n\n    # -- Roll out Hubble Relay pods automatically when configmap is updated.\n    rollOutPods: false\n\n    # -- TLS configuration for Hubble Relay\n    tls:\n      # -- base64 encoded PEM values for the hubble-relay client certificate and private key\n      # This keypair is presented to Hubble server instances for mTLS\n      # authentication and is required when hubble.tls.enabled is true.\n      # These values need to be set manually if hubble.tls.auto.enabled is false.\n      client:\n        cert: \"\"\n        key: \"\"\n      # -- base64 encoded PEM values for the hubble-relay server certificate and private key\n      server:\n        # When set to true, enable TLS on for Hubble Relay server\n        # (ie: for clients connecting to the Hubble Relay API).\n        enabled: false\n        # These values need to be set manually if hubble.tls.auto.enabled is false.\n        cert: \"\"\n        key: \"\"\n\n    # -- Dial timeout to connect to the local hubble instance to receive peer information (e.g. \"30s\").\n    dialTimeout: ~\n\n    # -- Backoff duration to retry connecting to the local hubble instance in case of failure (e.g. \"30s\").\n    retryTimeout: ~\n\n    # -- Max number of flows that can be buffered for sorting before being sent to the\n    # client (per request) (e.g. 100).\n    sortBufferLenMax: ~\n\n    # -- When the per-request flows sort buffer is not full, a flow is drained every\n    # time this timeout is reached (only affects requests in follow-mode) (e.g. \"1s\").\n    sortBufferDrainTimeout: ~\n\n    # -- Port to use for the k8s service backed by hubble-relay pods.\n    # If not set, it is dynamically assigned to port 443 if TLS is enabled and to\n    # port 80 if not.\n    # servicePort: 80\n\n  ui:\n    # -- Whether to enable the Hubble UI.\n    enabled: true # kind\n\n    # -- hubble-ui ingress configuration.\n    ingress:\n      enabled: true # kind\n      annotations:\n        kubernetes.io/ingress.class: kong\n      hosts:\n        - hubble.local\n      tls: []\n      #  - secretName: chart-example-tls\n      #    hosts:\n      #      - chart-example.local\n\n# -- Configure whether to install iptables rules to allow for TPROXY\n# (L7 proxy injection), iptables-based masquerading and compatibility\n# with kube-proxy.\ninstallIptablesRules: true\n\n# -- Install Iptables rules to skip netfilter connection tracking on all pod\n# traffic. This option is only effective when Cilium is running in direct\n# routing and full KPR mode. Moreover, this option cannot be enabled when Cilium\n# is running in a managed Kubernetes environment or in a chained CNI setup.\ninstallNoConntrackIptablesRules: false\n\nipam:\n  # -- Configure IP Address Management mode.\n  # ref: https://docs.cilium.io/en/stable/concepts/networking/ipam/\n  mode: \"kubernetes\" # kind\n  operator:\n    # -- IPv4 CIDR range to delegate to individual nodes for IPAM.\n    clusterPoolIPv4PodCIDR: \"10.0.0.0/8\"\n    # -- IPv4 CIDR mask size to delegate to individual nodes for IPAM.\n    clusterPoolIPv4MaskSize: 24\n    # -- IPv6 CIDR range to delegate to individual nodes for IPAM.\n    clusterPoolIPv6PodCIDR: \"fd00::/104\"\n    # -- IPv6 CIDR mask size to delegate to individual nodes for IPAM.\n    clusterPoolIPv6MaskSize: 120\n\n# -- Configure the eBPF-based ip-masq-agent\nipMasqAgent:\n  enabled: false\n\n# iptablesLockTimeout defines the iptables \"--wait\" option when invoked from Cilium.\n# iptablesLockTimeout: \"5s\"\n\n# -- Configure Kubernetes specific configuration\nk8s: {}\n  # -- requireIPv4PodCIDR enables waiting for Kubernetes to provide the PodCIDR\n  # range via the Kubernetes node resource\n  # requireIPv4PodCIDR: false\n\n  # -- requireIPv6PodCIDR enables waiting for Kubernetes to provide the PodCIDR\n  # range via the Kubernetes node resource\n  # requireIPv6PodCIDR: false\n\nstartupProbe:\n  # -- failure threshold of startup probe.\n  # 105 x 2s translates to the old behaviour of the readiness probe (120s delay + 30 x 3s)\n  failureThreshold: 105\n  # -- interval between checks of the startup probe\n  periodSeconds: 2\n\n# -- Configure the kube-proxy replacement in Cilium BPF datapath\n# Valid options are \"disabled\", \"probe\", \"partial\", \"strict\".\n# ref: https://docs.cilium.io/en/stable/gettingstarted/kubeproxy-free/\n#kubeProxyReplacement: \"partial\" # kind\nkubeProxyReplacement: \"strict\" # remove kube-proxy\n\n# -- Enable Layer 7 network policy.\nl7Proxy: true\n\n# logOptions allows you to define logging options. eg:\n# logOptions:\n#   format: json\n\n# -- Enables periodic logging of system load\nlogSystemLoad: false\n\n\n# -- Configure maglev consistent hashing\nmaglev: {}\n  # -- tableSize is the size (parameter M) for the backend table of one\n  # service entry\n  # tableSize:\n\n  # -- hashSeed is the cluster-wide base64 encoded seed for the hashing\n  # hashSeed:\n\n# -- Enables egress gateway (beta) to redirect and SNAT the traffic that\n# leaves the cluster.\negressGateway:\n  enabled: false\n\n# -- Specify the CIDR for native routing (ie to avoid IP masquerade for).\n# This value corresponds to the configured cluster-cidr.\n# nativeRoutingCIDR:\n\nmonitor:\n  # -- Enable the cilium-monitor sidecar.\n  enabled: false\n\n# -- Configure N-S k8s service loadbalancing\nnodePort:\n  # -- Enable the Cilium NodePort service implementation.\n  enabled: true # kind\n\n  # -- Port range to use for NodePort services.\n  # range: \"30000,32767\"\n\n# policyAuditMode: false\n\n# -- The agent can be put into one of the three policy enforcement modes:\n# default, always and never.\n# ref: https://docs.cilium.io/en/stable/policy/intro/#policy-enforcement-modes\npolicyEnforcementMode: \"default\"\n\npprof:\n  # -- Enable Go pprof debugging\n  enabled: false\n\n# -- Configure prometheus metrics on the configured port at /metrics\nprometheus:\n  enabled: false\n  port: 9090\n  serviceMonitor:\n    # -- Enable service monitors.\n    # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/master/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)\n    #\n    enabled: true\n    # -- Specify the Kubernetes namespace where Prometheus expects to find\n    # service monitors configured.\n    # namespace: \"\"\n  # -- Metrics that should be enabled or disabled from the default metric\n  # list. (+metric_foo to enable metric_foo , -metric_bar to disable\n  # metric_bar).\n  # ref: https://docs.cilium.io/en/stable/operations/metrics/#exported-metrics\n  metrics: ~\n\n# -- Configure Istio proxy options.\nproxy:\n  prometheus:\n    enabled: true\n    port: \"9095\"\n  # -- Regular expression matching compatible Istio sidecar istio-proxy\n  # container image names\n  sidecarImageRegex: \"cilium/istio_proxy\"\n\n# -- Enable use of the remote node identity.\n# ref: https://docs.cilium.io/en/v1.7/install/upgrade/#configmap-remote-node-identity\nremoteNodeIdentity: true\n\n# -- Enable resource quotas for priority classes used in the cluster.\nresourceQuotas:\n  enabled: false\n  cilium:\n    hard:\n      # 5k nodes * 2 DaemonSets (Cilium and cilium node init)\n      pods: \"10k\"\n  operator:\n    hard:\n      # 15 \"clusterwide\" Cilium Operator pods for HA\n      pods: \"15\"\n\n# -- Configure BPF socket operations configuration\nsockops:\n  # enabled enables installation of socket options acceleration.\n  enabled: false\n\n# -- Configure TLS configuration in the agent.\ntls:\n  enabled: true\n  secretsBackend: local\n\n# -- Configure the encapsulation configuration for communication between nodes.\n# Possible values:\n#   - disabled\n#   - vxlan (default)\n#   - geneve\ntunnel: \"vxlan\"\n\nwellKnownIdentities:\n  # -- Enable the use of well-known identities.\n  enabled: false\n\noperator:\n  # -- Enable the cilium-operator component (required).\n  enabled: true\n\n  # -- Roll out cilium-operator pods automatically when configmap is updated.\n  rollOutPods: false\n\n  # -- Enable prometheus metrics for cilium-operator on the configured port at\n  # /metrics\n  prometheus:\n    enabled: false\n    port: 6942\n    serviceMonitor:\n      # -- Enable service monitors.\n      # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/master/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)\n      ##\n      enabled: true\n\nnodeinit:\n  # -- Enable the node initialization DaemonSet\n  enabled: true # kind\n\n  # -- The priority class to use for the nodeinit pod.\n  priorityClassName: \"\"\n\nclustermesh:\n  # -- Deploy clustermesh-apiserver for clustermesh\n  useAPIServer: false\n\n  apiserver:\n    # -- Clustermesh API server image.\n    service:\n      # -- The type of service used for apiserver access.\n      type: NodePort\n      # -- Optional port to use as the node port for apiserver access.\n      nodePort: 32379\n      # -- Optional loadBalancer IP address to use with type LoadBalancer.\n      # loadBalancerIP:\n\n      # -- Annotations for the clustermesh-apiserver\n      # For GKE LoadBalancer, use annotation cloud.google.com/load-balancer-type: \"Internal\"\n      # For EKS LoadBalancer, use annotation service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0\n      annotations: {}\n\n    tls:\n      # -- Configure automatic TLS certificates generation.\n      # A Kubernetes CronJob is used the generate any\n      # certificates not provided by the user at installation\n      # time.\n      auto:\n        # -- When set to true, automatically generate a CA and certificates to\n        # enable mTLS between clustermesh-apiserver and external workload instances.\n        # If set to false, the certs to be provided by setting appropriate values below.\n        enabled: true\n        # Sets the method to auto-generate certificates. Supported values:\n        # - helm:      This method uses Helm to generate all certificates.\n        # - cronJob:   This method uses a Kubernetes CronJob the generate any\n        #              certificates not provided by the user at installation\n        #              time.\n        method: helm\n        # -- Generated certificates validity duration in days.\n        certValidityDuration: 1095\n        # -- Schedule for certificates regeneration (regardless of their expiration date).\n        # Only used if method is \"cronJob\". If nil, then no recurring job will be created.\n        # Instead, only the one-shot job is deployed to generate the certificates at\n        # installation time.\n        #\n        # Due to the out-of-band distribution of client certs to external workloads the\n        # CA is (re)regenerated only if it is not provided as a helm value and the k8s\n        # secret is manually deleted.\n        #\n        # Defaults to none. Commented syntax gives midnight of the first day of every\n        # fourth month. For syntax, see\n        # https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#schedule\n        # schedule: \"0 0 1 */4 *\"\n      # -- base64 encoded PEM values for the ExternalWorkload CA certificate and private key.\n      ca:\n        # -- Optional CA cert. If it is provided, it will be used by the 'cronJob' method to\n        # generate all other certificates. Otherwise, an ephemeral CA is generated.\n        cert: \"\"\n        # -- Optional CA private key. If it is provided, it will be used by the 'cronJob' method to\n        # generate all other certificates. Otherwise, an ephemeral CA is generated.\n        key: \"\"\n      # -- base64 encoded PEM values for the clustermesh-apiserver server certificate and private key.\n      # Used if 'auto' is not enabled.\n      server:\n        cert: \"\"\n        key: \"\"\n      # -- base64 encoded PEM values for the clustermesh-apiserver admin certificate and private key.\n      # Used if 'auto' is not enabled.\n      admin:\n        cert: \"\"\n        key: \"\"\n      # -- base64 encoded PEM values for the clustermesh-apiserver client certificate and private key.\n      # Used if 'auto' is not enabled.\n      client:\n        cert: \"\"\n        key: \"\"\n      # -- base64 encoded PEM values for the clustermesh-apiserver remote cluster certificate and private key.\n      # Used if 'auto' is not enabled.\n      remote:\n        cert: \"\"\n        key: \"\"\n\n# -- Configure cgroup related configuration\ncgroup:\n  autoMount:\n    # -- Enable auto mount of cgroup2 filesystem.\n    # When `autoMount` is enabled, cgroup2 filesystem is mounted at\n    # `cgroup.hostRoot` path on the underlying host and inside the cilium agent pod.\n    # If users disable `autoMount`, it's expected that users have mounted\n    # cgroup2 filesystem at the specified `cgroup.hostRoot` volume, and then the\n    # volume will be mounted inside the cilium agent pod at the same path.\n    enabled: true\n  # -- Configure cgroup root where cgroup2 filesystem is mounted on the host (see also: `cgroup.autoMount`)\n  hostRoot: /run/cilium/cgroupv2\n"
            ],
            "verify": false,
            "version": "1.10.3",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "kong",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": true,
            "chart": "kong",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "kong",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 10,
            "metadata": [
              {
                "app_version": "2.5",
                "chart": "kong",
                "name": "kong",
                "namespace": "kong",
                "revision": 1,
                "values": "{\"proxy\":{\"http\":{\"enabled\":true,\"nodePort\":32080},\"ingress\":{\"enabled\":false},\"tls\":{\"enabled\":true,\"nodePort\":32443},\"type\":\"NodePort\"},\"serviceMonitor\":{\"enabled\":true,\"namespace\":\"kong\"},\"status\":{\"enabled\":true}}",
                "version": "2.3.0"
              }
            ],
            "name": "kong",
            "namespace": "kong",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://charts.konghq.com",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "status:\n  enabled: true\n\nproxy:\n  # Enable creating a Kubernetes service for the proxy\n  type: NodePort #ClusterIP # LoadBalancer does not work with kind\n  \n  ingress:\n    # Enable/disable exposure using ingress.\n    enabled: false # true causing errors\n\n  http:\n    # Enable plaintext HTTP listen for the proxy\n    enabled: true\n    # Set a nodePort which is available if service type is NodePort\n    nodePort: 32080\n\n  tls:\n    # Enable HTTPS listen for the proxy\n    enabled: true\n    # Set a nodePort which is available if service type is NodePort\n    nodePort: 32443\n\nserviceMonitor:\n  # Specifies whether ServiceMonitor for Prometheus operator should be created\n  # If you wish to gather metrics from a Kong instance with the proxy disabled (such as a hybrid control plane), see:\n  # https://github.com/Kong/charts/blob/main/charts/kong/README.md#prometheus-operator-integration\n  enabled: true\n  # interval: 10s\n  # Specifies namespace, where ServiceMonitor should be installed\n  namespace: kong\n  # labels:\n  #   foo: bar\n  # targetLabels:\n  #   - foo\n\n  # honorLabels: false\n  # metricRelabelings: []\n"
            ],
            "verify": false,
            "version": "2.3.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "helm_release.prometheus",
            "kubernetes_namespace.kong",
            "kubernetes_namespace.prometheus"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "localstack",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "localstack",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "localstack",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 10,
            "metadata": [
              {
                "app_version": "latest",
                "chart": "localstack",
                "name": "localstack",
                "namespace": "localstack",
                "revision": 1,
                "values": "{\"debug\":false,\"ingress\":{\"annotations\":{\"kubernetes.io/ingress.class\":\"kong\"},\"enabled\":false,\"hosts\":[{\"host\":\"kong.local\",\"paths\":[]}],\"tls\":[]},\"mountDind\":{\"enabled\":false,\"forceTLS\":true,\"image\":\"docker:20.10-dind\"},\"podSecurityContext\":{},\"securityContext\":{},\"service\":{\"apiServices\":[{\"name\":\"es\",\"nodePort\":31571,\"targetPort\":4571}],\"edgeService\":{\"name\":\"edge\",\"nodePort\":31566,\"targetPort\":4566},\"type\":\"ClusterIP\"}}",
                "version": "0.2.3"
              }
            ],
            "name": "localstack",
            "namespace": "localstack",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "http://helm.localstack.cloud",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "podSecurityContext: {}\n  # fsGroup: 2000\n\nsecurityContext: {}\n  # capabilities:\n  #   drop:\n  #   - ALL\n  # readOnlyRootFilesystem: true\n  # runAsNonRoot: true\n  # runAsUser: 1000\n\ndebug: false\n\n# This will enable the Docker daemon binding and allow\n# Localstack to provide Lambdas and other AWS services\n# who got container runtime dependencies\nmountDind:\n  enabled: false\n  forceTLS: true\n  image: \"docker:20.10-dind\"\n\nservice:\n  type: ClusterIP #NodePort\n  edgeService:\n    name: edge\n    targetPort: 4566\n    nodePort: 31566\n  apiServices:\n    - name: es\n      targetPort: 4571\n      nodePort: 31571\n\ningress:\n  enabled: false # error\n  annotations:\n    kubernetes.io/ingress.class: kong\n  hosts:\n    - host: kong.local\n      paths: []\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\n"
            ],
            "verify": false,
            "version": "0.2.3",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "kubernetes_namespace.localstack"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "metrics_server",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": true,
            "chart": "metrics-server",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "metrics-server",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 10,
            "metadata": [
              {
                "app_version": "0.5.0",
                "chart": "metrics-server",
                "name": "metrics-server",
                "namespace": "kube-system",
                "revision": 1,
                "values": "{\"apiService\":{\"create\":true},\"command\":[\"metrics-server\"],\"extraArgs\":{\"kubelet-insecure-tls\":true,\"kubelet-preferred-address-types\":\"InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP\"},\"hostNetwork\":false,\"securePort\":8443}",
                "version": "5.9.3"
              }
            ],
            "name": "metrics-server",
            "namespace": "kube-system",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://charts.bitnami.com/bitnami",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "apiService:\n  ## @param apiService.create Specifies whether the v1beta1.metrics.k8s.io API service should be created. You can check if it is needed with `kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\"`.\n  ## This is still necessary up to at least k8s version \u003e= 1.21, but depends on vendors and cloud providers.\n  ##\n  create: true # kind\n## @param securePort Port where metrics-server will be running\n##\nsecurePort: 8443\n## @param hostNetwork Enable hostNetwork mode\n## You would require this enabled if you use alternate overlay networking for pods and\n## API server unable to communicate with metrics-server. As an example, this is required\n## if you use Weave network on EKS\n##\nhostNetwork: false\n## @param command Override default container command (useful when using custom images)\n##\ncommand: [\"metrics-server\"]\n## @param extraArgs Extra arguments to pass to metrics-server on start up\n## ref: https://github.com/kubernetes-incubator/metrics-server/blob/master/README.md#flags\n##\nextraArgs:\n  kubelet-insecure-tls: true\n  kubelet-preferred-address-types: InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP\n"
            ],
            "verify": false,
            "version": "5.9.3",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "prometheus",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "kube-prometheus-stack",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "prometheus",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 10,
            "metadata": [
              {
                "app_version": "0.50.0",
                "chart": "kube-prometheus-stack",
                "name": "prometheus",
                "namespace": "prometheus",
                "revision": 1,
                "values": "{\"alertmanager\":{\"enabled\":false},\"global\":{\"rbac\":{\"pspAnnotations\":{}}},\"grafana\":{\"adminPassword\":\"admin\",\"forceDeployDashboards\":false,\"forceDeployDatasources\":false,\"ingress\":{\"annotations\":{\"kubernetes.io/ingress.class\":\"kong\"},\"enabled\":true,\"hosts\":[],\"labels\":{},\"path\":\"/\",\"tls\":[]},\"sidecar\":{\"dashboards\":{\"annotations\":{},\"enabled\":true,\"label\":\"grafana_dashboard\",\"multicluster\":{\"etcd\":{\"enabled\":false},\"global\":{\"enabled\":false}}},\"datasources\":{\"annotations\":{},\"createPrometheusReplicasDatasources\":false,\"defaultDatasourceEnabled\":true,\"enabled\":true,\"label\":\"grafana_datasource\"}}},\"kube-state-metrics\":{\"podSecurityPolicy\":{\"enabled\":false}},\"prometheus\":{\"additionalPodMonitors\":[],\"additionalRulesForClusterRole\":[],\"additionalServiceMonitors\":[],\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"ingressPerReplica\":{\"annotations\":{},\"enabled\":false,\"hostDomain\":\"\",\"hostPrefix\":\"\",\"labels\":{},\"paths\":[],\"tlsSecretName\":\"\",\"tlsSecretPerReplica\":{\"enabled\":false,\"prefix\":\"prometheus\"}},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"podSecurityPolicy\":{\"allowedCapabilities\":[],\"allowedHostPaths\":[],\"volumes\":[]},\"prometheusSpec\":{\"additionalAlertManagerConfigs\":[],\"additionalAlertManagerConfigsSecret\":{},\"additionalAlertRelabelConfigs\":[],\"additionalPrometheusSecretsAnnotations\":{},\"additionalRemoteRead\":[],\"additionalRemoteWrite\":[],\"additionalScrapeConfigs\":[],\"additionalScrapeConfigsSecret\":{},\"affinity\":{},\"alertingEndpoints\":[],\"allowOverlappingBlocks\":false,\"apiserverConfig\":{},\"arbitraryFSAccessThroughSMs\":false,\"configMaps\":[],\"containers\":[],\"disableCompaction\":false,\"enableAdminAPI\":false,\"enableFeatures\":[],\"enforcedNamespaceLabel\":\"\",\"enforcedSampleLimit\":false,\"evaluationInterval\":\"\",\"externalLabels\":{},\"externalUrl\":\"\",\"ignoreNamespaceSelectors\":false,\"image\":{\"repository\":\"quay.io/prometheus/prometheus\",\"sha\":\"\",\"tag\":\"v2.28.1\"},\"initContainers\":[],\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"nodeSelector\":{},\"overrideHonorLabels\":false,\"overrideHonorTimestamps\":false,\"paused\":false,\"podAntiAffinity\":\"\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"podMonitorNamespaceSelector\":{},\"podMonitorSelector\":{},\"podMonitorSelectorNilUsesHelmValues\":true,\"portName\":\"web\",\"priorityClassName\":\"\",\"probeNamespaceSelector\":{},\"probeSelector\":{},\"probeSelectorNilUsesHelmValues\":true,\"prometheusExternalLabelName\":\"\",\"prometheusExternalLabelNameClear\":false,\"prometheusRulesExcludedFromEnforce\":[],\"query\":{},\"queryLogFile\":false,\"remoteRead\":[],\"remoteWrite\":[],\"remoteWriteDashboards\":false,\"replicaExternalLabelName\":\"\",\"replicaExternalLabelNameClear\":false,\"replicas\":1,\"resources\":{},\"retention\":\"10d\",\"retentionSize\":\"\",\"routePrefix\":\"/\",\"ruleNamespaceSelector\":{},\"ruleSelector\":{},\"ruleSelectorNilUsesHelmValues\":true,\"scrapeInterval\":\"\",\"scrapeTimeout\":\"\",\"secrets\":[],\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000},\"serviceMonitorNamespaceSelector\":{},\"serviceMonitorSelector\":{},\"serviceMonitorSelectorNilUsesHelmValues\":true,\"shards\":1,\"storageSpec\":{},\"thanos\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[],\"walCompression\":false,\"web\":{}},\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30090,\"port\":9090,\"sessionAffinity\":\"\",\"targetPort\":9090,\"type\":\"ClusterIP\"},\"serviceMonitor\":{\"bearerTokenFile\":null,\"interval\":\"\",\"metricRelabelings\":[],\"relabelings\":[],\"scheme\":\"\",\"selfMonitor\":true,\"tlsConfig\":{}},\"servicePerReplica\":{\"annotations\":{},\"enabled\":false,\"loadBalancerSourceRanges\":[],\"nodePort\":30091,\"port\":9090,\"targetPort\":9090,\"type\":\"ClusterIP\"},\"thanosIngress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"nodePort\":30901,\"paths\":[],\"servicePort\":10901,\"tls\":[]},\"thanosService\":{\"annotations\":{},\"clusterIP\":\"None\",\"enabled\":false,\"labels\":{},\"nodePort\":30901,\"port\":10901,\"portName\":\"grpc\",\"targetPort\":\"grpc\",\"type\":\"ClusterIP\"},\"thanosServiceExternal\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30901,\"port\":10901,\"portName\":\"grpc\",\"targetPort\":\"grpc\",\"type\":\"LoadBalancer\"}},\"prometheusOperator\":{\"admissionWebhooks\":{\"caBundle\":\"\",\"certManager\":{\"enabled\":false},\"enabled\":false,\"patch\":{\"enabled\":true,\"securityContext\":{\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":2000}}},\"hostNetwork\":false,\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534},\"tls\":{\"enabled\":false,\"tlsMinVersion\":\"VersionTLS13\"}}}",
                "version": "18.0.1"
              }
            ],
            "name": "prometheus",
            "namespace": "prometheus",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://prometheus-community.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "global:\n  rbac:\n    pspAnnotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n## Configuration for alertmanager\n## ref: https://prometheus.io/docs/alerting/alertmanager/\n##\nalertmanager:\n  enabled: false # don't need alerts in kind\n\n## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml\n##\ngrafana:\n  ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDatasources: false\n\n  ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDashboards: false\n\n  adminPassword: admin\n\n  ingress:\n    ## If true, Grafana Ingress will be created\n    ##\n    enabled: true\n\n    ## Annotations for Grafana Ingress\n    ##\n    annotations:\n      kubernetes.io/ingress.class: kong\n\n    ## Labels to be added to the Ingress\n    ##\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enable.\n    ##\n    # hosts:\n    #   - grafana.domain.com\n    hosts: []\n\n    ## Path for grafana ingress\n    path: /\n\n    ## TLS configuration for grafana Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: grafana-general-tls\n    #   hosts:\n    #   - grafana.example.com\n\n  sidecar:\n    dashboards:\n      enabled: true\n      label: grafana_dashboard\n\n      ## Annotations for Grafana dashboard configmaps\n      ##\n      annotations: {}\n      multicluster:\n        global:\n          enabled: false\n        etcd:\n          enabled: false\n    datasources:\n      enabled: true\n      defaultDatasourceEnabled: true\n\n      ## URL of prometheus datasource\n      ##\n      # url: http://prometheus-stack-prometheus:9090/\n\n      # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default\n      # defaultDatasourceScrapeInterval: 15s\n\n      ## Annotations for Grafana datasource configmaps\n      ##\n      annotations: {}\n\n      ## Create datasource for each Pod of Prometheus StatefulSet;\n      ## this uses headless service `prometheus-operated` which is\n      ## created by Prometheus Operator\n      ## ref: https://git.io/fjaBS\n      createPrometheusReplicasDatasources: false\n      label: grafana_datasource\n\n\n## Manages Prometheus and Alertmanager components\n##\nprometheusOperator:\n  ## Prometheus-Operator v0.39.0 and later support TLS natively.\n  ##\n  tls:\n    # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants\n    tlsMinVersion: VersionTLS13\n    enabled: false # avoid tls error\n\n  admissionWebhooks:\n    enabled: false # avoid patch error\n    ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.\n    ## If unspecified, system trust roots on the apiserver are used.\n    caBundle: \"\"\n    ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.\n    ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own\n    ## certs ahead of time if you wish.\n    ##\n    patch:\n      enabled: true # kind\n      ## SecurityContext holds pod-level security attributes and common container settings.\n      ## This defaults to non root user with uid 2000 and gid 2000. *v1.PodSecurityContext  false\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n      ##\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n\n    # Use certmanager to generate webhook certs\n    certManager:\n      enabled: false # failed to call webhook: Post \"https://cert-manager-webhook.cert-manager.svc:443/mutate?timeout=10s\"\n      # issuerRef:\n      #   name: \"issuer\"\n      #   kind: \"ClusterIssuer\"\n\n  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n  ##\n  hostNetwork: false # kind\n\n  securityContext:\n    fsGroup: 65534\n    runAsGroup: 65534\n    runAsNonRoot: true\n    runAsUser: 65534\n\n## Deploy a Prometheus instance\n##\nprometheus:\n  # Service for thanos service discovery on sidecar\n  # Enable this can make Thanos Query can use\n  # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery\n  # Thanos sidecar on prometheus nodes\n  # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)\n  thanosService:\n    enabled: false\n    annotations: {}\n    labels: {}\n    portName: grpc\n    port: 10901\n    targetPort: \"grpc\"\n    clusterIP: \"None\"\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n    ## Port to expose on each node\n    ##\n    nodePort: 30901\n\n  # Service for external access to sidecar\n  # Enabling this creates a service to expose thanos-sidecar outside the cluster.\n  thanosServiceExternal:\n    enabled: false\n    annotations: {}\n    labels: {}\n    portName: grpc\n    port: 10901\n    targetPort: \"grpc\"\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Service type\n    ##\n    type: LoadBalancer\n\n    ## Port to expose on each node\n    ##\n    nodePort: 30901\n\n  ## Configuration for Prometheus service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n    ## Port for Prometheus Service to listen on\n    ##\n    port: 9090\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9090\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30090\n\n    ## Loadbalancer IP\n    ## Only use if service.type is \"LoadBalancer\"\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    ## Service type\n    ##\n    type: ClusterIP\n\n    sessionAffinity: \"\"\n\n  ## Configuration for creating a separate Service for each statefulset Prometheus replica\n  ##\n  servicePerReplica:\n    enabled: false\n    annotations: {}\n\n    ## Port for Prometheus Service per replica to listen on\n    ##\n    port: 9090\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9090\n\n    ## Port to expose on each node\n    ## Only used if servicePerReplica.type is 'NodePort'\n    ##\n    nodePort: 30091\n\n    ## Loadbalancer source IP ranges\n    ## Only used if servicePerReplica.type is \"LoadBalancer\"\n    loadBalancerSourceRanges: []\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## Configure pod disruption budgets for Prometheus\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ## This configuration is immutable once created and will require the PDB to be deleted to be changed\n  ## https://github.com/kubernetes/kubernetes/issues/45398\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  # Ingress exposes thanos sidecar outside the cluster\n  thanosIngress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n    servicePort: 10901\n\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30901\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n      # - thanos-gateway.domain.com\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Thanos Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: thanos-gateway-tls\n    #   hosts:\n    #   - thanos-gateway.domain.com\n    #\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n  ingress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enabled.\n    ##\n    # hosts:\n    #   - prometheus.domain.com\n    hosts: []\n\n    ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Prometheus Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n      # - secretName: prometheus-general-tls\n      #   hosts:\n      #     - prometheus.example.com\n\n  ## Configuration for creating an Ingress that will map to each Prometheus replica service\n  ## prometheus.servicePerReplica must be enabled\n  ##\n  ingressPerReplica:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Final form of the hostname for each per replica ingress is\n    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}\n    ##\n    ## Prefix for the per replica ingress that will have `-$replicaNumber`\n    ## appended to the end\n    hostPrefix: \"\"\n    ## Domain that will be used for the per replica ingress\n    hostDomain: \"\"\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## Secret name containing the TLS certificate for Prometheus per replica ingress\n    ## Secret must be manually created in the namespace\n    tlsSecretName: \"\"\n\n    ## Separated secret for each per replica Ingress. Can be used together with cert-manager\n    ##\n    tlsSecretPerReplica:\n      enabled: false\n      ## Final form of the secret for each per replica ingress is\n      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}\n      ##\n      prefix: \"prometheus\"\n\n  ## Configure additional options for default pod security policy for Prometheus\n  ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  podSecurityPolicy:\n    allowedCapabilities: []\n    allowedHostPaths: []\n    volumes: []\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    selfMonitor: true\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    #   relabel configs to apply to samples before ingestion.\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n  ## Settings affecting prometheusSpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#prometheusspec\n  ##\n  prometheusSpec:\n    ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos\n    ##\n    disableCompaction: false\n    ## APIServerConfig\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#apiserverconfig\n    ##\n    apiserverConfig: {}\n\n    ## Interval between consecutive scrapes.\n    ## Defaults to 30s.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183\n    ##\n    scrapeInterval: \"\"\n\n    ## Number of seconds to wait for target to respond before erroring\n    ##\n    scrapeTimeout: \"\"\n\n    ## Interval between consecutive evaluations.\n    ##\n    evaluationInterval: \"\"\n\n    ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.\n    ##\n    listenLocal: false\n\n    ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.\n    ## This is disabled by default.\n    ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis\n    ##\n    enableAdminAPI: false\n\n    ## WebTLSConfig defines the TLS parameters for HTTPS\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#webtlsconfig\n    web: {}\n\n    # EnableFeatures API enables access to Prometheus disabled features.\n    # ref: https://prometheus.io/docs/prometheus/latest/disabled_features/\n    enableFeatures: []\n    # - exemplar-storage\n\n    ## Image of Prometheus.\n    ##\n    image:\n      repository: quay.io/prometheus/prometheus\n      tag: v2.28.1\n      sha: \"\"\n\n    ## Tolerations for use with node taints\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    #  - key: \"key\"\n    #    operator: \"Equal\"\n    #    value: \"value\"\n    #    effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: prometheus\n\n    ## Alertmanagers to which alerts will be sent\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#alertmanagerendpoints\n    ##\n    ## Default configuration will connect to the alertmanager deployed as part of this release\n    ##\n    alertingEndpoints: []\n    # - name: \"\"\n    #   namespace: \"\"\n    #   port: http\n    #   scheme: http\n    #   pathPrefix: \"\"\n    #   tlsConfig: {}\n    #   bearerTokenFile: \"\"\n    #   apiVersion: v2\n\n    ## External labels to add to any time series or alerts when communicating with external systems\n    ##\n    externalLabels: {}\n\n    ## Name of the external label used to denote replica name\n    ##\n    replicaExternalLabelName: \"\"\n\n    ## If true, the Operator won't add the external label used to denote replica name\n    ##\n    replicaExternalLabelNameClear: false\n\n    ## Name of the external label used to denote Prometheus instance name\n    ##\n    prometheusExternalLabelName: \"\"\n\n    ## If true, the Operator won't add the external label used to denote Prometheus instance name\n    ##\n    prometheusExternalLabelNameClear: false\n\n    ## External URL at which Prometheus will be reachable.\n    ##\n    externalUrl: \"\"\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not\n    ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated\n    ## with the new list of secrets.\n    ##\n    secrets: []\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.\n    ##\n    configMaps: []\n\n    ## QuerySpec defines the query command line flags when starting Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#queryspec\n    ##\n    query: {}\n\n    ## Namespaces to be selected for PrometheusRules discovery.\n    ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage\n    ##\n    ruleNamespaceSelector: {}\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the PrometheusRule resources created\n    ##\n    ruleSelectorNilUsesHelmValues: true\n\n    ## PrometheusRules to be selected for target discovery.\n    ## If {}, select all PrometheusRules\n    ##\n    ruleSelector: {}\n    ## Example which select all PrometheusRules resources\n    ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\"\n    # ruleSelector:\n    #   matchExpressions:\n    #     - key: prometheus\n    #       operator: In\n    #       values:\n    #         - example-rules\n    #         - example-rules-2\n    #\n    ## Example which select all PrometheusRules resources with label \"role\" set to \"example-rules\"\n    # ruleSelector:\n    #   matchLabels:\n    #     role: example-rules\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the servicemonitors created\n    ##\n    serviceMonitorSelectorNilUsesHelmValues: true\n\n    ## ServiceMonitors to be selected for target discovery.\n    ## If {}, select all ServiceMonitors\n    ##\n    serviceMonitorSelector: {}\n    ## Example which selects ServiceMonitors with label \"prometheus\" set to \"somelabel\"\n    # serviceMonitorSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## Namespaces to be selected for ServiceMonitor discovery.\n    ##\n    serviceMonitorNamespaceSelector: {}\n    ## Example which selects ServiceMonitors in namespaces with label \"prometheus\" set to \"somelabel\"\n    # serviceMonitorNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the podmonitors created\n    ##\n    podMonitorSelectorNilUsesHelmValues: true\n\n    ## PodMonitors to be selected for target discovery.\n    ## If {}, select all PodMonitors\n    ##\n    podMonitorSelector: {}\n    ## Example which selects PodMonitors with label \"prometheus\" set to \"somelabel\"\n    # podMonitorSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## Namespaces to be selected for PodMonitor discovery.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage\n    ##\n    podMonitorNamespaceSelector: {}\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the probes created\n    ##\n    probeSelectorNilUsesHelmValues: true\n\n    ## Probes to be selected for target discovery.\n    ## If {}, select all Probes\n    ##\n    probeSelector: {}\n    ## Example which selects Probes with label \"prometheus\" set to \"somelabel\"\n    # probeSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## Namespaces to be selected for Probe discovery.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage\n    ##\n    probeNamespaceSelector: {}\n\n    ## How long to retain metrics\n    ##\n    retention: 10d\n\n    ## Maximum size of metrics\n    ##\n    retentionSize: \"\"\n\n    ## Enable compression of the write-ahead log using Snappy.\n    ##\n    walCompression: false\n\n    ## If true, the Operator won't process any Prometheus configuration changes\n    ##\n    paused: false\n\n    ## Number of replicas of each shard to deploy for a Prometheus deployment.\n    ## Number of replicas multiplied by shards is the total number of Pods created.\n    ##\n    replicas: 1\n\n    ## EXPERIMENTAL: Number of shards to distribute targets onto.\n    ## Number of replicas multiplied by shards is the total number of Pods created.\n    ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved.\n    ## Increasing shards will not reshard data either but it will continue to be available from the same instances.\n    ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location.\n    ## Sharding is done on the content of the `__address__` target meta-label.\n    ##\n    shards: 1\n\n    ## Log level for Prometheus be configured in\n    ##\n    logLevel: info\n\n    ## Log format for Prometheus be configured in\n    ##\n    logFormat: logfmt\n\n    ## Prefix used to register routes, overriding externalUrl route.\n    ## Useful for proxies that rewrite URLs.\n    ##\n    routePrefix: /\n\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the prometheus pods.\n    ##\n    podMetadata: {}\n    # labels:\n    #   app: prometheus\n    #   k8s-app: prometheus\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    podAntiAffinity: \"\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the prometheus instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## The remote_read spec configuration for Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#remotereadspec\n    remoteRead: []\n    # - url: http://remote1/read\n    ## additionalRemoteRead is appended to remoteRead\n    additionalRemoteRead: []\n\n    ## The remote_write spec configuration for Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#remotewritespec\n    remoteWrite: []\n    # - url: http://remote1/push\n    ## additionalRemoteWrite is appended to remoteWrite\n    additionalRemoteWrite: []\n\n    ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature\n    remoteWriteDashboards: false\n\n    ## Resource limits \u0026 requests\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Prometheus StorageSpec for persistent data\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/storage.md\n    ##\n    storageSpec: {}\n    ## Using PersistentVolumeClaim\n    ##\n    #  volumeClaimTemplate:\n    #    spec:\n    #      storageClassName: gluster\n    #      accessModes: [\"ReadWriteOnce\"]\n    #      resources:\n    #        requests:\n    #          storage: 50Gi\n    #    selector: {}\n\n    ## Using tmpfs volume\n    ##\n    #  emptyDir:\n    #    medium: Memory\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations\n    ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form\n    ## as specified in the official Prometheus documentation:\n    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are\n    ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility\n    ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible\n    ## scrape configs are going to break Prometheus after the upgrade.\n    ##\n    ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the\n    ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes\n    ##\n    additionalScrapeConfigs: []\n    # - job_name: kube-etcd\n    #   kubernetes_sd_configs:\n    #     - role: node\n    #   scheme: https\n    #   tls_config:\n    #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n    #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n    #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n    #   relabel_configs:\n    #   - action: labelmap\n    #     regex: __meta_kubernetes_node_label_(.+)\n    #   - source_labels: [__address__]\n    #     action: replace\n    #     targetLabel: __address__\n    #     regex: ([^:;]+):(\\d+)\n    #     replacement: ${1}:2379\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: keep\n    #     regex: .*mst.*\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: replace\n    #     targetLabel: node\n    #     regex: (.*)\n    #     replacement: ${1}\n    #   metric_relabel_configs:\n    #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)\n    #     action: labeldrop\n\n    ## If additional scrape configurations are already deployed in a single secret file you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalScrapeConfigs\n    additionalScrapeConfigsSecret: {}\n      # enabled: false\n      # name:\n      # key:\n\n    ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful\n    ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'\n    additionalPrometheusSecretsAnnotations: {}\n\n    ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified\n    ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#\u003calertmanager_config\u003e.\n    ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.\n    ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this\n    ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release\n    ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertManagerConfigs: []\n    # - consul_sd_configs:\n    #   - server: consul.dev.test:8500\n    #     scheme: http\n    #     datacenter: dev\n    #     tag_separator: ','\n    #     services:\n    #       - metrics-prometheus-alertmanager\n\n    ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage\n    ## them separately from the helm deployment, you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalAlertManagerConfigs\n    additionalAlertManagerConfigsSecret: {}\n      # name:\n      # key:\n\n    ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended\n    ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the\n    ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.\n    ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the\n    ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel\n    ## configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertRelabelConfigs: []\n    # - separator: ;\n    #   regex: prometheus_replica\n    #   replacement: $1\n    #   action: labeldrop\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.\n    ## This section is experimental, it may change significantly without deprecation notice in any release.\n    ## This is experimental and may change significantly without backward compatibility in any release.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#thanosspec\n    ##\n    thanos: {}\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.\n    ## if using proxy extraContainer update targetPort with proxy container port\n    containers: []\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## PortName to use for Prometheus.\n    ##\n    portName: \"web\"\n\n    ## ArbitraryFSAccessThroughSMs configures whether configuration based on a service monitor can access arbitrary files\n    ## on the file system of the Prometheus container e.g. bearer token files.\n    arbitraryFSAccessThroughSMs: false\n\n    ## OverrideHonorLabels if set to true overrides all user configured honor_labels. If HonorLabels is set in ServiceMonitor\n    ## or PodMonitor to true, this overrides honor_labels to false.\n    overrideHonorLabels: false\n\n    ## OverrideHonorTimestamps allows to globally enforce honoring timestamps in all scrape configs.\n    overrideHonorTimestamps: false\n\n    ## IgnoreNamespaceSelectors if set to true will ignore NamespaceSelector settings from the podmonitor and servicemonitor\n    ## configs, and they will only discover endpoints within their current namespace. Defaults to false.\n    ignoreNamespaceSelectors: false\n\n    ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created.\n    ## The label value will always be the namespace of the object that is being created.\n    ## Disabled by default\n    enforcedNamespaceLabel: \"\"\n\n    ## PrometheusRulesExcludedFromEnforce - list of prometheus rules to be excluded from enforcing of adding namespace labels.\n    ## Works only if enforcedNamespaceLabel set to true. Make sure both ruleNamespace and ruleName are set for each pair\n    prometheusRulesExcludedFromEnforce: []\n\n    ## QueryLogFile specifies the file to which PromQL queries are logged. Note that this location must be writable,\n    ## and can be persisted using an attached volume. Alternatively, the location can be set to a stdout location such\n    ## as /dev/stdout to log querie information to the default Prometheus log stream. This is only available in versions\n    ## of Prometheus \u003e= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/)\n    queryLogFile: false\n\n    ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit\n    ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall\n    ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead.\n    enforcedSampleLimit: false\n\n    ## AllowOverlappingBlocks enables vertical compaction and vertical query merge in Prometheus. This is still experimental\n    ## in Prometheus so it may change in any upcoming release.\n    allowOverlappingBlocks: false\n\n  additionalRulesForClusterRole: []\n  #  - apiGroups: [ \"\" ]\n  #    resources:\n  #      - nodes/proxy\n  #    verbs: [ \"get\", \"list\", \"watch\" ]\n\n  additionalServiceMonitors: []\n  ## Name of the ServiceMonitor to create\n  ##\n  # - name: \"\"\n\n    ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from\n    ## the chart\n    ##\n    # additionalLabels: {}\n\n    ## Service label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\n    ## If no label is specified, the service name is used.\n    ##\n    # jobLabel: \"\"\n\n    ## labels to transfer from the kubernetes service to the target\n    ##\n    # targetLabels: []\n\n    ## labels to transfer from the kubernetes pods to the target\n    ##\n    # podTargetLabels: []\n\n    ## Label selector for services to which this ServiceMonitor applies\n    ##\n    # selector: {}\n\n    ## Namespaces from which services are selected\n    ##\n    # namespaceSelector:\n      ## Match any namespace\n      ##\n      # any: false\n\n      ## Explicit list of namespace names to select\n      ##\n      # matchNames: []\n\n    ## Endpoints of the selected service to be monitored\n    ##\n    # endpoints: []\n      ## Name of the endpoint's service port\n      ## Mutually exclusive with targetPort\n      # - port: \"\"\n\n      ## Name or number of the endpoint's target port\n      ## Mutually exclusive with port\n      # - targetPort: \"\"\n\n      ## File containing bearer token to be used when scraping targets\n      ##\n      #   bearerTokenFile: \"\"\n\n      ## Interval at which metrics should be scraped\n      ##\n      #   interval: 30s\n\n      ## HTTP path to scrape for metrics\n      ##\n      #   path: /metrics\n\n      ## HTTP scheme to use for scraping\n      ##\n      #   scheme: http\n\n      ## TLS configuration to use when scraping the endpoint\n      ##\n      #   tlsConfig:\n\n          ## Path to the CA file\n          ##\n          # caFile: \"\"\n\n          ## Path to client certificate file\n          ##\n          # certFile: \"\"\n\n          ## Skip certificate verification\n          ##\n          # insecureSkipVerify: false\n\n          ## Path to client key file\n          ##\n          # keyFile: \"\"\n\n          ## Server name used to verify host name\n          ##\n          # serverName: \"\"\n\n  additionalPodMonitors: []\n  ## Name of the PodMonitor to create\n  ##\n  # - name: \"\"\n\n    ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from\n    ## the chart\n    ##\n    # additionalLabels: {}\n\n    ## Pod label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\n    ## If no label is specified, the pod endpoint name is used.\n    ##\n    # jobLabel: \"\"\n\n    ## Label selector for pods to which this PodMonitor applies\n    ##\n    # selector: {}\n\n    ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.\n    ##\n    # podTargetLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    # sampleLimit: 0\n\n    ## Namespaces from which pods are selected\n    ##\n    # namespaceSelector:\n      ## Match any namespace\n      ##\n      # any: false\n\n      ## Explicit list of namespace names to select\n      ##\n      # matchNames: []\n\n    ## Endpoints of the selected pods to be monitored\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#podmetricsendpoint\n    ##\n    # podMetricsEndpoints: []\n\nkube-state-metrics:\n  podSecurityPolicy:\n    enabled: false # deprecated in 1.21\n"
            ],
            "verify": false,
            "version": "18.0.1",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "kubernetes_namespace.prometheus"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "cert_manager",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "cert-manager",
            "metadata": [
              {
                "annotations": null,
                "generate_name": "",
                "generation": 0,
                "labels": null,
                "name": "cert-manager",
                "resource_version": "563",
                "uid": "48d4e4ea-c717-4e71-a5c9-83186d13d939"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "kong",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "kong",
            "metadata": [
              {
                "annotations": null,
                "generate_name": "",
                "generation": 0,
                "labels": null,
                "name": "kong",
                "resource_version": "561",
                "uid": "e83b5310-0414-42f5-9858-b877bff25ad9"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "localstack",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "localstack",
            "metadata": [
              {
                "annotations": null,
                "generate_name": "",
                "generation": 0,
                "labels": null,
                "name": "localstack",
                "resource_version": "562",
                "uid": "5ff73caa-b68b-4d62-8f47-cbf5fe1bbe32"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "prometheus",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "prometheus",
            "metadata": [
              {
                "annotations": null,
                "generate_name": "",
                "generation": 0,
                "labels": null,
                "name": "prometheus",
                "resource_version": "564",
                "uid": "bb6c4d4e-72f4-4079-a23e-c7f550e3eafd"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    }
  ]
}
